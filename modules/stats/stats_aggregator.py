"""
IRC Statistics - Aggregator Module (Thread-Safe Version)
=========================================================
ProceseazƒÉ evenimentele din IRC_EVENTS »ôi le agregƒÉ √Æn tabelele de statistici.

‚úÖ Thread-safe cu locks corecte
‚úÖ State persistence pentru recovery
‚úÖ Error recovery cu retry logic
‚úÖ Metrics tracking
‚úÖ Configurable limits

FIXES:
- ‚úÖ Thread safety cu threading.Lock() »ôi RLock()
- ‚úÖ State management atomic
- ‚úÖ Retry logic pentru DB errors
- ‚úÖ Better error handling
- ‚úÖ Metrics pentru monitoring
- ‚úÖ Configuration externalizatƒÉ
"""

from __future__ import annotations

import time
import datetime
import re
import threading
from collections import defaultdict
from typing import Optional, Dict, List, Tuple, Any
from contextlib import contextmanager
from core.log import get_logger

logger = get_logger("stats_aggregator")


# =============================================================================
# Configuration
# =============================================================================
class AggregatorConfig:
    """Configura»õie pentru aggregator - thread-safe singleton"""

    _instance = None
    _lock = threading.Lock()

    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if self._initialized:
            return

        # Reply detection
        self.reply_window_seconds = 120

        # Batch processing
        self.max_events_per_batch = 5000
        self.max_retries = 3
        self.retry_delay = 1.0  # seconds

        # Emotion detection (compile regex odatƒÉ)
        self.smile_re = re.compile(r"(:\)|:-\)|=\)|:D|:-D|=\]|;\)|;-\)|\^_\^|üòä|üòÉ|üòÑ|üôÇ)")
        self.sad_re = re.compile(r"(:\(|:-\(|=\(|üò¢|üò≠|‚òπÔ∏è|üôÅ)")
        self.laugh_re = re.compile(r"(xD|XD|üòÇ|ü§£|lol\b|lmao\b|rofl\b)", re.IGNORECASE)
        self.angry_re = re.compile(r"(:@|>:\(|üò°|üò†)")
        self.heart_re = re.compile(r"(‚ù§Ô∏è|‚ô•|<3)")

        self._initialized = True
        logger.info("AggregatorConfig initialized")

    def update(self, **kwargs):
        """Thread-safe config update"""
        with self._lock:
            for key, value in kwargs.items():
                if hasattr(self, key):
                    setattr(self, key, value)
                    logger.info(f"Config updated: {key} = {value}")


# =============================================================================
# Helpers
# =============================================================================
def _is_valid_channel(channel: str | None) -> bool:
    """ValideazƒÉ cƒÉ e un canal IRC (#channel)"""
    return bool(channel) and isinstance(channel, str) and channel.startswith("#")


def _count_emotions(message: str | None, config: AggregatorConfig) -> dict:
    """NumƒÉrƒÉ emo»õii √Æn mesaj folosind regex pre-compilate"""
    if not message:
        return {"smiles": 0, "sads": 0, "laughs": 0, "angries": 0, "hearts": 0}

    return {
        "smiles": len(config.smile_re.findall(message)),
        "sads": len(config.sad_re.findall(message)),
        "laughs": len(config.laugh_re.findall(message)),
        "angries": len(config.angry_re.findall(message)),
        "hearts": len(config.heart_re.findall(message)),
    }

_WORD_RE = re.compile(r"[A-Za-z0-9_']{2,}")

def _extract_words(message: str) -> list[str]:
    """Extract words for STATS_WORDS_DAILY aggregation."""
    if not message:
        return []
    msg = str(message).strip().lower()
    if not msg:
        return []
    words = _WORD_RE.findall(msg)
    out = []
    for w in words:
        if w.isdigit():
            continue
        out.append(w)
    return out

# -----------------------------------------------------------------------------
# Helpers for emoji counting (used by records)
# -----------------------------------------------------------------------------

_EMOJI_RE = re.compile(
    "["  # common emoji ranges
    "\U0001F300-\U0001FAFF"
    "\U00002600-\U000027BF"
    "]",
    flags=re.UNICODE
)

def _count_emojis(message: str) -> int:
    """Count emojis in message (rough but fast)."""
    if not message:
        return 0
    return len(_EMOJI_RE.findall(str(message)))



# =============================================================================
# Thread-Safe Stats Aggregator
# =============================================================================
class StatsAggregator:
    """
    Agregator incremental thread-safe pentru statistici IRC.

    Thread Safety Features:
    - RLock pentru opera»õii complexe (permite re-entrancy)
    - Atomic state updates
    - Lock-free reads pentru metrics
    - State persistence pentru recovery
    """

    def __init__(self, sql_instance):
        self.sql = sql_instance
        self.config = AggregatorConfig()

        # Thread-safe state management
        self._state_lock = threading.RLock()  # Reentrant lock pentru opera»õii complexe
        self._metrics_lock = threading.Lock()  # Separate lock pentru metrics

        # Processing state
        self._last_processed_id = 0
        self._is_running = False
        self._current_batch_id = 0

        # Metrics (read-only pentru debugging)
        self._total_events_processed = 0
        self._total_batches_processed = 0
        self._total_errors = 0
        self._last_run_time = None
        self._last_run_duration = 0.0
        self._last_run_events = 0

        # State recovery: load last processed ID from DB
        self._load_processing_state()

        logger.info(
            f"StatsAggregator initialized "
            f"(last_processed_id={self._last_processed_id})"
        )

    # =========================================================================
    # State Management (Thread-Safe)
    # =========================================================================

    def _load_processing_state(self) -> None:
        """
        Load last processed ID from database pentru recovery.
        Folosim max(id) din STATS_DAILY ca checkpoint.
        """
        try:
            # Query pentru ultimul event procesat (inferƒÉm din max id √Æn STATS_DAILY)
            query = """
                SELECT MAX(id) FROM IRC_EVENTS
                WHERE id <= (
                    SELECT MAX(last_event_id) FROM (
                        SELECT id as last_event_id FROM STATS_DAILY 
                        ORDER BY id DESC LIMIT 1
                    )
                )
            """
            result = self.sql.sqlite_select(query, ())

            if result and result[0][0] is not None:
                self._last_processed_id = int(result[0][0])
                logger.info(f"Recovered processing state: last_id={self._last_processed_id}")

        except Exception as e:
            logger.warning(f"Could not load processing state: {e}")
            self._last_processed_id = 0

    def _save_processing_state(self) -> None:
        """
        SalveazƒÉ state-ul de procesare (op»õional - folosim checkpoint implicit √Æn STATS_DAILY).
        AceastƒÉ metodƒÉ poate fi extinsƒÉ cu un tabel dedicat AGGREGATOR_STATE.
        """
        # Pentru viitor: CREATE TABLE AGGREGATOR_STATE (key TEXT PRIMARY KEY, value TEXT)
        pass

    @contextmanager
    def _state_context(self):
        """Context manager pentru opera»õii cu state lock"""
        self._state_lock.acquire()
        try:
            yield
        finally:
            self._state_lock.release()

    def _update_metrics(self, events_count: int, duration: float, success: bool) -> None:
        """Thread-safe metrics update"""
        with self._metrics_lock:
            self._total_batches_processed += 1
            self._last_run_time = time.time()
            self._last_run_duration = duration
            self._last_run_events = events_count

            if success:
                self._total_events_processed += events_count
            else:
                self._total_errors += 1

    # =========================================================================
    # Main Aggregation Logic (Thread-Safe)
    # =========================================================================

    def aggregate_all(self, max_events: Optional[int] = None) -> dict:
        """
        ProceseazƒÉ evenimente noi din IRC_EVENTS »ôi le agregƒÉ.

        Thread-Safe: Folose»ôte lock pentru a preveni procesarea simultanƒÉ.

        Returns:
            dict: Status info (success/error, events_processed, duration, etc)
        """
        # ‚úÖ THREAD SAFETY: Check »ôi set is_running atomic
        with self._state_context():
            if self._is_running:
                logger.warning("Aggregation already running, skipping")
                return {
                    "status": "skipped",
                    "reason": "already_running",
                    "current_batch_id": self._current_batch_id,
                }

            self._is_running = True
            self._current_batch_id += 1
            batch_id = self._current_batch_id

        start_time = time.time()
        events_processed = 0

        try:
            # Use config limit if not specified
            if max_events is None:
                max_events = self.config.max_events_per_batch

            logger.info(f"[Batch #{batch_id}] Starting aggregation (max_events={max_events})")

            # ‚úÖ Fetch events - atomic read of last_processed_id
            with self._state_context():
                last_id = self._last_processed_id

            events = self._fetch_new_events(last_id, max_events)

            if not events:
                duration = time.time() - start_time
                self._update_metrics(0, duration, True)

                logger.debug(f"[Batch #{batch_id}] No new events to process")
                return {
                    "status": "success",
                    "batch_id": batch_id,
                    "events_processed": 0,
                    "duration": duration,
                }

            events_processed = len(events)
            logger.info(f"[Batch #{batch_id}] Processing {events_processed} events...")

            # ‚úÖ Process events with retry logic
            self._process_events_with_retry(events, batch_id)

            # ‚úÖ THREAD SAFETY: Update last_processed_id atomic
            with self._state_context():
                new_last_id = max(e["id"] for e in events)
                self._last_processed_id = new_last_id

            duration = time.time() - start_time
            self._update_metrics(events_processed, duration, True)

            logger.info(
                f"[Batch #{batch_id}] ‚úÖ Complete: {events_processed} events "
                f"in {duration:.2f}s ({events_processed/duration:.1f} ev/s)"
            )

            return {
                "status": "success",
                "batch_id": batch_id,
                "events_processed": events_processed,
                "last_id": new_last_id,
                "duration": duration,
                "rate": events_processed / duration if duration > 0 else 0,
            }

        except Exception as e:
            duration = time.time() - start_time
            self._update_metrics(events_processed, duration, False)

            logger.error(
                f"[Batch #{batch_id}] ‚ùå Failed: {e}",
                exc_info=True
            )

            return {
                "status": "error",
                "batch_id": batch_id,
                "error": str(e),
                "error_type": type(e).__name__,
                "events_processed": events_processed,
                "duration": duration,
            }

        finally:
            # ‚úÖ THREAD SAFETY: Reset is_running atomic
            with self._state_context():
                self._is_running = False

    def _process_events_with_retry(self, events: List[dict], batch_id: int) -> None:
        """
        Process events cu retry logic pentru DB errors.
        """
        max_retries = self.config.max_retries
        retry_delay = self.config.retry_delay

        for attempt in range(max_retries):
            try:
                # Process all aggregation steps
                self._process_events(events)
                return  # Success

            except Exception as e:
                if attempt < max_retries - 1:
                    logger.warning(
                        f"[Batch #{batch_id}] Attempt {attempt + 1}/{max_retries} failed: {e}. "
                        f"Retrying in {retry_delay}s..."
                    )
                    time.sleep(retry_delay)
                    retry_delay *= 2  # Exponential backoff
                else:
                    # Final attempt failed
                    raise

    def _process_events(self, events: List[dict]) -> None:
        """
        Process events - toate step-urile de agregare.
        AceastƒÉ metodƒÉ poate arunca excep»õii care vor fi caught de retry logic.
        """
        # 1) DAILY stats
        daily_stats = self._group_events_daily(events)
        self._update_daily_stats(daily_stats)

        # 2) HOURLY stats
        self._update_hourly_stats(events)

        # 3) CHANNEL summary
        self._update_channel_stats(events)

        # 4) REPLY PAIRS
        self._update_reply_pairs(events)

        # 5) NEW: words / last spoken / records
        self._update_words_daily(events)
        self._update_last_spoken(events)
        self._update_channel_records(events)

        # 6) NICK activity
        self._update_nick_activity(events)

    # =========================================================================
    # Fetch Events
    # =========================================================================

    def _fetch_new_events(self, last_processed_id: int, limit: int) -> List[dict]:
        """
        Fetch evenimente noi din IRC_EVENTS.

        Args:
            last_processed_id: Last processed event ID
            limit: Max events to fetch

        Returns:
            List of event dicts
        """
        query = """
            SELECT id, botId, channel, ts, event_type, nick, ident, host,
                   message, target_nick, reason, mode_change,
                   words, chars, is_question, is_exclamation, has_url, is_caps
            FROM IRC_EVENTS
            WHERE id > ?
            ORDER BY id ASC
            LIMIT ?
        """

        rows = self.sql.sqlite_select(query, (last_processed_id, limit))

        events = []
        for row in rows:
            events.append({
                "id": row[0],
                "botId": row[1],
                "channel": row[2],
                "ts": row[3],
                "event_type": row[4],
                "nick": row[5],
                "ident": row[6],
                "host": row[7],
                "message": row[8],
                "target_nick": row[9],
                "reason": row[10],
                "mode_change": row[11],
                "words": row[12] or 0,
                "chars": row[13] or 0,
                "is_question": row[14] or 0,
                "is_exclamation": row[15] or 0,
                "has_url": row[16] or 0,
                "is_caps": row[17] or 0,
            })

        return events

    # =========================================================================
    # DAILY Stats Aggregation
    # =========================================================================

    def _group_events_daily(self, events: List[dict]) -> dict:
        """
        GrupeazƒÉ evenimente per (botId, channel, nick, date).
        """
        daily = defaultdict(lambda: {
            "messages": 0,
            "actions": 0,
            "words": 0,
            "chars": 0,
            "questions": 0,
            "exclamations": 0,
            "urls": 0,
            "caps_msgs": 0,
            "smiles": 0,
            "sads": 0,
            "laughs": 0,
            "angries": 0,
            "hearts": 0,
            "joins": 0,
            "parts": 0,
            "quits": 0,
            "kicks_received": 0,
            "kicks_given": 0,
            "first_seen_ts": None,
            "last_seen_ts": None,
        })

        for event in events:
            if not _is_valid_channel(event["channel"]):
                continue

            date_str = datetime.datetime.fromtimestamp(event["ts"]).strftime("%Y-%m-%d")
            key = (event["botId"], event["channel"], event["nick"], date_str)
            stats = daily[key]

            et = event["event_type"]

            # Message types
            if et == "PRIVMSG":
                stats["messages"] += 1
                stats["words"] += event["words"]
                stats["chars"] += event["chars"]
                stats["questions"] += event["is_question"]
                stats["exclamations"] += event["is_exclamation"]
                stats["urls"] += event["has_url"]
                stats["caps_msgs"] += event["is_caps"]

                # Emotions
                emotions = _count_emotions(event["message"], self.config)
                for emo_key, count in emotions.items():
                    stats[emo_key] += count

            elif et == "ACTION":
                stats["actions"] += 1
                stats["words"] += event["words"]
                stats["chars"] += event["chars"]

                # Emotions in actions too
                emotions = _count_emotions(event["message"], self.config)
                for emo_key, count in emotions.items():
                    stats[emo_key] += count

            elif et == "JOIN":
                stats["joins"] += 1

            elif et == "PART":
                stats["parts"] += 1

            elif et == "QUIT":
                stats["quits"] += 1

            elif et == "KICK":
                if event["target_nick"] == event["nick"]:
                    stats["kicks_received"] += 1
                else:
                    stats["kicks_given"] += 1

            # Timestamp tracking
            ts = event["ts"]
            if stats["first_seen_ts"] is None or ts < stats["first_seen_ts"]:
                stats["first_seen_ts"] = ts
            if stats["last_seen_ts"] is None or ts > stats["last_seen_ts"]:
                stats["last_seen_ts"] = ts

        return daily



    # =========================================================================
    # NEW: WORDS DAILY
    # =========================================================================
    def _update_words_daily(self, events: List[dict]) -> None:
        """
        PopuleazƒÉ STATS_WORDS_DAILY (top words per zi/canal).
        """
        if not events:
            return

        counts: Dict[Tuple[int, str, str, str], int] = defaultdict(int)
        # key = (botId, channel, date, word)

        for e in events:
            if not _is_valid_channel(e.get("channel")):
                continue
            if e.get("event_type") not in ("PRIVMSG", "ACTION"):
                continue
            msg = e.get("message") or ""
            if not msg:
                continue

            date_str = datetime.datetime.fromtimestamp(e["ts"]).strftime("%Y-%m-%d")
            bot_id = int(e["botId"])
            channel = str(e["channel"])

            for w in _extract_words(msg):
                counts[(bot_id, channel, date_str, w)] += 1

        if not counts:
            return

        upsert_sql = """
            INSERT INTO STATS_WORDS_DAILY (botId, channel, date, word, count)
            VALUES (?, ?, ?, ?, ?)
            ON CONFLICT(botId, channel, date, word) DO UPDATE SET
                count = count + excluded.count
        """

        # executƒÉm √Æn batch
        rows = [(k[0], k[1], k[2], k[3], v) for k, v in counts.items()]
        for r in rows:
            self.sql.sqlite3_insert(upsert_sql, r)

    # =========================================================================
    # NEW: LAST SPOKEN
    # =========================================================================
    def _update_last_spoken(self, events: List[dict]) -> None:
        """
        PopuleazƒÉ STATS_LAST_SPOKEN (ultimul mesaj + ultimul cuv√¢nt).
        """

        def _last_line_preview(message: str, max_words: int = 5) -> str:
            """
            ReturneazƒÉ primele max_words cuvinte din mesaj + '...' dacƒÉ e mai lung.
            """
            if not message:
                return ""

            msg = message.strip()
            if not msg:
                return ""

            words = re.split(r"\s+", msg)
            if len(words) <= max_words:
                return msg

            return " ".join(words[:max_words]) + "..."
        if not events:
            return

        latest: Dict[Tuple[int, str, str], Tuple[int, str, str]] = {}
        # key=(botId, channel, nick) -> (ts, message, last_word)

        for e in events:
            if not _is_valid_channel(e.get("channel")):
                continue
            if e.get("event_type") not in ("PRIVMSG", "ACTION"):
                continue

            msg = e.get("message") or ""
            if not msg:
                continue

            key = (int(e["botId"]), str(e["channel"]), str(e["nick"]))
            ts = int(e["ts"])
            lw = _last_line_preview(msg)

            cur = latest.get(key)
            if (cur is None) or (ts >= cur[0]):
                latest[key] = (ts, msg, lw)

        if not latest:
            return

        upsert_sql = """
            INSERT INTO STATS_LAST_SPOKEN (botId, channel, nick, last_ts, last_message, last_word)
            VALUES (?, ?, ?, ?, ?, ?)
            ON CONFLICT(botId, channel, nick) DO UPDATE SET
                last_ts = excluded.last_ts,
                last_message = excluded.last_message,
                last_word = excluded.last_word
            WHERE excluded.last_ts >= STATS_LAST_SPOKEN.last_ts
        """

        for (bot_id, channel, nick), (ts, msg, lw) in latest.items():
            self.sql.sqlite3_insert(upsert_sql, (bot_id, channel, nick, ts, msg, lw))

    # =========================================================================
    # NEW: CHANNEL RECORDS
    # =========================================================================
    def _update_channel_records(self, events: List[dict]) -> None:
        """
        PopuleazƒÉ STATS_CHANNEL_RECORDS:
        - longest line (chars)
        - most emojis in one line
        - peak minute (max messages in same minute)
        """
        if not events:
            return


        # 1) candidate records per (botId, channel)
        best_longest: Dict[Tuple[int, str], Tuple[int, str, int, str]] = {}
        # -> (chars, nick, ts, message)
        best_emojis: Dict[Tuple[int, str], Tuple[int, str, int, str]] = {}
        # -> (emoji_count, nick, ts, message)

        # 2) peak minute counts
        per_minute: Dict[Tuple[int, str, int], int] = defaultdict(int)
        # key=(botId, channel, minute_ts) minute_ts = (ts//60)*60

        for e in events:
            if not _is_valid_channel(e.get("channel")):
                continue
            if e.get("event_type") not in ("PRIVMSG", "ACTION"):
                continue

            bot_id = int(e["botId"])
            channel = str(e["channel"])
            nick = str(e["nick"])
            ts = int(e["ts"])
            msg = e.get("message") or ""
            if not msg:
                continue

            key = (bot_id, channel)

            # longest
            ch = len(msg)
            cur_l = best_longest.get(key)
            if (cur_l is None) or (ch > cur_l[0]):
                best_longest[key] = (ch, nick, ts, msg)

            # most emojis
            em = _count_emojis(msg)
            cur_e = best_emojis.get(key)
            if (cur_e is None) or (em > cur_e[0]):
                best_emojis[key] = (em, nick, ts, msg)

            # peak minute
            minute_ts = (ts // 60) * 60
            per_minute[(bot_id, channel, minute_ts)] += 1

        # Ensure rows exist
        for (bot_id, channel) in set(list(best_longest.keys()) + list(best_emojis.keys())):
            self.sql.sqlite3_insert(
                "INSERT OR IGNORE INTO STATS_CHANNEL_RECORDS (botId, channel) VALUES (?, ?)",
                (bot_id, channel)
            )

        # Update longest / most emojis (only if better than existing)
        for (bot_id, channel), (chars, nick, ts, msg) in best_longest.items():
            self.sql.sqlite3_update("""
                UPDATE STATS_CHANNEL_RECORDS
                SET longest_chars = ?,
                    longest_nick = ?,
                    longest_ts = ?,
                    longest_message = ?
                WHERE botId = ? AND channel = ?
                  AND (longest_chars IS NULL OR longest_chars = 0 OR ? > longest_chars)
            """, (chars, nick, ts, msg, bot_id, channel, chars))

        for (bot_id, channel), (cnt, nick, ts, msg) in best_emojis.items():
            self.sql.sqlite3_update("""
                UPDATE STATS_CHANNEL_RECORDS
                SET most_emojis = ?,
                    most_emojis_nick = ?,
                    most_emojis_ts = ?,
                    most_emojis_message = ?
                WHERE botId = ? AND channel = ?
                  AND (most_emojis IS NULL OR most_emojis = 0 OR ? > most_emojis)
            """, (cnt, nick, ts, msg, bot_id, channel, cnt))

        # Peak minute: gƒÉse»ôte max per (botId, channel) √Æn batch
        peak_best: Dict[Tuple[int, str], Tuple[int, int, str]] = {}
        # -> (count, minute_ts, label)

        for (bot_id, channel, minute_ts), cnt in per_minute.items():
            key = (bot_id, channel)
            label = datetime.datetime.fromtimestamp(minute_ts).strftime("%Y-%m-%d %H:%M")
            cur = peak_best.get(key)
            if (cur is None) or (cnt > cur[0]):
                peak_best[key] = (cnt, minute_ts, label)

        for (bot_id, channel), (cnt, mts, label) in peak_best.items():
            self.sql.sqlite3_insert(
                "INSERT OR IGNORE INTO STATS_CHANNEL_RECORDS (botId, channel) VALUES (?, ?)",
                (bot_id, channel)
            )
            self.sql.sqlite3_update("""
                UPDATE STATS_CHANNEL_RECORDS
                SET peak_minute_count = ?,
                    peak_minute_ts = ?,
                    peak_minute_label = ?
                WHERE botId = ? AND channel = ?
                  AND (peak_minute_count IS NULL OR peak_minute_count = 0 OR ? > peak_minute_count)
            """, (cnt, mts, label, bot_id, channel, cnt))


    def _update_daily_stats(self, daily_stats: dict) -> None:
        """
        Update STATS_DAILY table cu INSERT ... ON CONFLICT DO UPDATE.
        """
        if not daily_stats:
            return

        upsert_sql = """
            INSERT INTO STATS_DAILY (
                botId, channel, nick, date,
                messages, actions, words, chars,
                questions, exclamations, urls, caps_msgs,
                smiles, sads, laughs, angries, hearts,
                joins, parts, quits, kicks_received, kicks_given,
                first_seen_ts, last_seen_ts
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ON CONFLICT(botId, channel, nick, date) DO UPDATE SET
                messages = messages + excluded.messages,
                actions = actions + excluded.actions,
                words = words + excluded.words,
                chars = chars + excluded.chars,
                questions = questions + excluded.questions,
                exclamations = exclamations + excluded.exclamations,
                urls = urls + excluded.urls,
                caps_msgs = caps_msgs + excluded.caps_msgs,
                smiles = smiles + excluded.smiles,
                sads = sads + excluded.sads,
                laughs = laughs + excluded.laughs,
                angries = angries + excluded.angries,
                hearts = hearts + excluded.hearts,
                joins = joins + excluded.joins,
                parts = parts + excluded.parts,
                quits = quits + excluded.quits,
                kicks_received = kicks_received + excluded.kicks_received,
                kicks_given = kicks_given + excluded.kicks_given,
                first_seen_ts = MIN(STATS_DAILY.first_seen_ts, excluded.first_seen_ts),
                last_seen_ts = MAX(STATS_DAILY.last_seen_ts, excluded.last_seen_ts)
        """

        for (botId, channel, nick, date), stats in daily_stats.items():
            try:
                self.sql.sqlite3_insert(upsert_sql, (
                    botId, channel, nick, date,
                    stats["messages"],
                    stats["actions"],
                    stats["words"],
                    stats["chars"],
                    stats["questions"],
                    stats["exclamations"],
                    stats["urls"],
                    stats["caps_msgs"],
                    stats["smiles"],
                    stats["sads"],
                    stats["laughs"],
                    stats["angries"],
                    stats["hearts"],
                    stats["joins"],
                    stats["parts"],
                    stats["quits"],
                    stats["kicks_received"],
                    stats["kicks_given"],
                    stats["first_seen_ts"],
                    stats["last_seen_ts"],
                ))

            except Exception as e:
                logger.error(
                    f"Error updating STATS_DAILY for {channel}/{nick}/{date}: {e}",
                    exc_info=True
                )

    def _update_nick_activity(self, events: List[dict]) -> None:
        """
        PopuleazƒÉ STATS_NICK_ACTIVITY pentru retention:
        - first_seen_ts = primul eveniment vƒÉzut √Æn canal
        - last_seen_ts  = ultimul eveniment vƒÉzut √Æn canal
        """
        if not events:
            return

        # key=(botId, channel, nick) -> (min_ts, max_ts)
        seen: Dict[Tuple[int, str, str], Tuple[int, int]] = {}

        for e in events:
            channel = e.get("channel")
            if not _is_valid_channel(channel):
                continue

            # considerƒÉm "activity" orice eveniment relevant de user √Æn canal
            # (po»õi restr√¢nge la PRIVMSG/ACTION dacƒÉ vrei)
            if e.get("event_type") not in ("PRIVMSG", "ACTION", "JOIN", "PART", "QUIT", "KICK", "NICK"):
                continue

            bot_id = int(e["botId"])
            nick = str(e.get("nick") or "").strip()
            if not nick:
                continue

            ts = int(e["ts"])
            key = (bot_id, str(channel), nick)

            cur = seen.get(key)
            if cur is None:
                seen[key] = (ts, ts)
            else:
                mn, mx = cur
                if ts < mn:
                    mn = ts
                if ts > mx:
                    mx = ts
                seen[key] = (mn, mx)

        if not seen:
            return

        upsert = """
                 INSERT INTO STATS_NICK_ACTIVITY (botId, channel, nick, first_seen_ts, last_seen_ts)
                 VALUES (?, ?, ?, ?, ?) ON CONFLICT(botId, channel, nick) DO \
                 UPDATE SET
                     first_seen_ts = CASE \
                     WHEN STATS_NICK_ACTIVITY.first_seen_ts IS NULL THEN excluded.first_seen_ts \
                     WHEN excluded.first_seen_ts < STATS_NICK_ACTIVITY.first_seen_ts THEN excluded.first_seen_ts \
                     ELSE STATS_NICK_ACTIVITY.first_seen_ts
                 END \
                 ,
                last_seen_ts = CASE
                    WHEN STATS_NICK_ACTIVITY.last_seen_ts IS NULL THEN excluded.last_seen_ts
                    WHEN excluded.last_seen_ts > STATS_NICK_ACTIVITY.last_seen_ts THEN excluded.last_seen_ts
                    ELSE STATS_NICK_ACTIVITY.last_seen_ts
                 END \
                 """

        for (bot_id, channel, nick), (mn, mx) in seen.items():
            self.sql.sqlite3_insert(upsert, (bot_id, channel, nick, mn, mx))

    # =========================================================================
    # HOURLY Stats (Heatmap)
    # =========================================================================

    def _update_hourly_stats(self, events: List[dict]) -> None:
        """
        Update STATS_HOURLY (heatmap: hour √ó day_of_week).
        """
        hourly = defaultdict(lambda: {"messages": 0, "users": set()})

        for event in events:
            if not _is_valid_channel(event["channel"]):
                continue

            if event["event_type"] not in ("PRIVMSG", "ACTION"):
                continue

            dt = datetime.datetime.fromtimestamp(event["ts"])
            hour = dt.hour
            dow = dt.weekday()  # 0=Monday, 6=Sunday

            key = (event["botId"], event["channel"], hour, dow)
            hourly[key]["messages"] += 1
            hourly[key]["users"].add(event["nick"])

        if not hourly:
            return

        upsert_sql = """
            INSERT INTO STATS_HOURLY (
                botId, channel, hour, day_of_week,
                total_messages, unique_users
            ) VALUES (?, ?, ?, ?, ?, ?)
            ON CONFLICT(botId, channel, hour, day_of_week) DO UPDATE SET
                total_messages = total_messages + excluded.total_messages,
                unique_users = MAX(STATS_HOURLY.unique_users, excluded.unique_users)
        """

        for (botId, channel, hour, dow), stats in hourly.items():
            try:
                self.sql.sqlite3_insert(upsert_sql, (
                    botId,
                    channel,
                    hour,
                    dow,
                    stats["messages"],
                    len(stats["users"]),
                ))

            except Exception as e:
                logger.error(
                    f"Error updating STATS_HOURLY for {channel} h{hour}dow{dow}: {e}",
                    exc_info=True
                )

    # =========================================================================
    # CHANNEL Summary Stats
    # =========================================================================

    def _update_channel_stats(self, events: List[dict]) -> None:
        """
        Update STATS_CHANNEL (channel summary).
        """
        channel_data = defaultdict(lambda: {
            "total_messages": 0,
            "total_words": 0,
            "nicks": set(),
            "first_ts": None,
            "last_ts": None,
        })

        for event in events:
            if not _is_valid_channel(event["channel"]):
                continue

            key = (event["botId"], event["channel"])
            data = channel_data[key]

            if event["event_type"] in ("PRIVMSG", "ACTION"):
                data["total_messages"] += 1
                data["total_words"] += event["words"]

            data["nicks"].add(event["nick"])

            ts = event["ts"]
            if data["first_ts"] is None or ts < data["first_ts"]:
                data["first_ts"] = ts
            if data["last_ts"] is None or ts > data["last_ts"]:
                data["last_ts"] = ts

        if not channel_data:
            return

        now = int(time.time())

        for (botId, channel), data in channel_data.items():
            try:
                # Check if exists
                select_sql = """
                    SELECT id, first_event_ts 
                    FROM STATS_CHANNEL 
                    WHERE botId = ? AND channel = ?
                """
                existing = self.sql.sqlite_select(select_sql, (botId, channel))

                if existing:
                    row_id = existing[0][0]
                    first_event_ts = existing[0][1]

                    # PƒÉstreazƒÉ first_event_ts minim
                    new_first = first_event_ts
                    if new_first is None:
                        new_first = data["first_ts"]
                    else:
                        new_first = min(int(new_first), int(data["first_ts"] or new_first))

                    update_sql = """
                        UPDATE STATS_CHANNEL SET
                            total_messages = total_messages + ?,
                            total_words = total_words + ?,
                            total_users = ?,
                            first_event_ts = ?,
                            last_event_ts = ?,
                            last_aggregated_ts = ?
                        WHERE id = ?
                    """

                    self.sql.sqlite3_update(update_sql, (
                        data["total_messages"],
                        data["total_words"],
                        len(data["nicks"]),
                        new_first,
                        data["last_ts"],
                        now,
                        row_id,
                    ))

                else:
                    # Insert new
                    insert_sql = """
                        INSERT INTO STATS_CHANNEL (
                            botId, channel,
                            total_messages, total_words, total_users,
                            top_talker_nick, top_talker_count,
                            most_active_day, most_active_hour,
                            first_event_ts, last_event_ts, last_aggregated_ts
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """

                    self.sql.sqlite3_insert(insert_sql, (
                        botId,
                        channel,
                        data["total_messages"],
                        data["total_words"],
                        len(data["nicks"]),
                        None, 0,  # top_talker_*
                        None, None,  # most_active_*
                        data["first_ts"],
                        data["last_ts"],
                        now,
                    ))

                # Update top talker (cumulative, from STATS_DAILY)
                top_sql = """
                    SELECT nick, SUM(messages + actions) AS total_msgs
                    FROM STATS_DAILY
                    WHERE botId = ? AND channel = ?
                    GROUP BY nick
                    ORDER BY total_msgs DESC
                    LIMIT 1
                """
                top = self.sql.sqlite_select(top_sql, (botId, channel))

                if top:
                    top_nick, top_count = top[0]
                    upd_top = """
                        UPDATE STATS_CHANNEL
                        SET top_talker_nick = ?, top_talker_count = ?
                        WHERE botId = ? AND channel = ?
                    """
                    self.sql.sqlite3_update(
                        upd_top,
                        (top_nick, int(top_count or 0), botId, channel)
                    )

            except Exception as e:
                logger.error(
                    f"Error updating STATS_CHANNEL for {channel}: {e}",
                    exc_info=True
                )

    # =========================================================================
    # REPLY PAIRS (Real nick-to-nick replies)
    # =========================================================================

    def _fetch_prev_message_state(
        self,
        botId: int,
        channel: str,
        before_id: int
    ) -> Optional[Tuple[str, int]]:
        """
        Pentru continuitate √Æntre batch-uri:
        ia ultimul PRIVMSG/ACTION √Ænainte de primul id procesat din canal.
        """
        q = """
            SELECT nick, ts
            FROM IRC_EVENTS
            WHERE botId = ? AND channel = ? AND id < ? 
              AND event_type IN ('PRIVMSG','ACTION')
            ORDER BY id DESC
            LIMIT 1
        """
        r = self.sql.sqlite_select(q, (botId, channel, before_id))

        if not r:
            return None

        return r[0][0], int(r[0][1])

    def _update_reply_pairs(self, events: List[dict]) -> None:
        """
        PopuleazƒÉ STATS_REPLY_PAIRS folosind heuristica:
        - doar PRIVMSG/ACTION
        - doar canale valide (#...)
        - dacƒÉ A vorbe»ôte dupƒÉ B √Æn <= REPLY_WINDOW_SECONDS »ôi A != B => A replied to B
        """
        if not events:
            return

        # GrupƒÉm evenimentele relevante per (botId, channel)
        per_chan = defaultdict(list)

        for e in events:
            if not _is_valid_channel(e["channel"]):
                continue
            if e["event_type"] not in ("PRIVMSG", "ACTION"):
                continue
            per_chan[(e["botId"], e["channel"])].append(e)

        if not per_chan:
            return

        # Ini»õializare state cu mesajul anterior (din DB) ca sƒÉ nu pierdem reply peste batch
        last_state = {}

        for (botId, channel), lst in per_chan.items():
            first_id = min(x["id"] for x in lst)
            prev = self._fetch_prev_message_state(int(botId), channel, int(first_id))

            if prev:
                last_state[(botId, channel)] = {"nick": prev[0], "ts": prev[1]}
            else:
                last_state[(botId, channel)] = None

        # AcumulƒÉm increments √Æn memorie, apoi upsert
        pairs = defaultdict(lambda: {"count": 0, "last_ts": 0})

        for (botId, channel), lst in per_chan.items():
            lst.sort(key=lambda x: x["id"])  # sigur cronologic
            prev = last_state.get((botId, channel))

            for e in lst:
                cur_nick = e["nick"]
                cur_ts = int(e["ts"])

                if prev:
                    prev_nick = prev["nick"]
                    prev_ts = int(prev["ts"])

                    if cur_nick and prev_nick and cur_nick != prev_nick:
                        if 0 <= (cur_ts - prev_ts) <= self.config.reply_window_seconds:
                            k = (botId, channel, cur_nick, prev_nick)  # from -> to
                            pairs[k]["count"] += 1
                            pairs[k]["last_ts"] = max(pairs[k]["last_ts"], cur_ts)

                prev = {"nick": cur_nick, "ts": cur_ts}

        if not pairs:
            return

        upsert_sql = """
            INSERT INTO STATS_REPLY_PAIRS
            (botId, channel, from_nick, to_nick, count, last_ts)
            VALUES (?, ?, ?, ?, ?, ?)
            ON CONFLICT(botId, channel, from_nick, to_nick)
            DO UPDATE SET
                count = count + excluded.count,
                last_ts = CASE
                    WHEN STATS_REPLY_PAIRS.last_ts IS NULL THEN excluded.last_ts
                    WHEN excluded.last_ts > STATS_REPLY_PAIRS.last_ts THEN excluded.last_ts
                    ELSE STATS_REPLY_PAIRS.last_ts
                END
        """

        for (botId, channel, from_nick, to_nick), agg in pairs.items():
            try:
                self.sql.sqlite3_insert(upsert_sql, (
                    botId,
                    channel,
                    from_nick,
                    to_nick,
                    int(agg["count"]),
                    int(agg["last_ts"] or 0),
                ))

            except Exception as e:
                logger.error(
                    f"Error updating STATS_REPLY_PAIRS for {channel} "
                    f"{from_nick}->{to_nick}: {e}",
                    exc_info=True
                )

    # =========================================================================
    # Status & Control
    # =========================================================================

    def get_status(self) -> Dict[str, Any]:
        """
        Thread-safe status info pentru monitoring.

        Returns:
            dict: Current aggregator status and metrics
        """
        with self._metrics_lock:
            metrics = {
                "total_events_processed": self._total_events_processed,
                "total_batches_processed": self._total_batches_processed,
                "total_errors": self._total_errors,
                "last_run_time": self._last_run_time,
                "last_run_duration": self._last_run_duration,
                "last_run_events": self._last_run_events,
            }

        with self._state_context():
            status = {
                "is_running": self._is_running,
                "last_processed_id": self._last_processed_id,
                "current_batch_id": self._current_batch_id,
            }

        return {
            **status,
            **metrics,
            "config": {
                "reply_window_seconds": self.config.reply_window_seconds,
                "max_events_per_batch": self.config.max_events_per_batch,
                "max_retries": self.config.max_retries,
            }
        }

    def reset_processing_state(self) -> None:
        """
        Reset processing state (pentru debugging sau manual recovery).
        ‚ö†Ô∏è Use with caution - poate cauza duplicate processing!
        """
        with self._state_context():
            self._last_processed_id = 0
            logger.warning("‚ö†Ô∏è Aggregator processing state RESET to 0")

    def get_metrics(self) -> Dict[str, Any]:
        """Quick metrics pentru monitoring"""
        with self._metrics_lock:
            return {
                "total_events": self._total_events_processed,
                "total_batches": self._total_batches_processed,
                "total_errors": self._total_errors,
                "avg_events_per_batch": (
                    self._total_events_processed / self._total_batches_processed
                    if self._total_batches_processed > 0 else 0
                ),
                "last_rate_per_sec": (
                    self._last_run_events / self._last_run_duration
                    if self._last_run_duration > 0 else 0
                ),
            }


# =============================================================================
# Periodic Runner (Thread-Safe Singleton)
# =============================================================================
_global_aggregator = None
_global_aggregator_lock = threading.Lock()


def get_aggregator(sql_instance) -> StatsAggregator:
    """
    Thread-safe singleton pentru aggregator.
    Folosit de periodic runner.
    """
    global _global_aggregator

    if _global_aggregator is None:
        with _global_aggregator_lock:
            if _global_aggregator is None:
                _global_aggregator = StatsAggregator(sql_instance)

    return _global_aggregator


def run_aggregation_periodic(sql_instance, interval_seconds: int = 300) -> None:
    """
    Func»õie rulatƒÉ periodic cu ThreadWorker.

    Thread-Safe: Folose»ôte singleton aggregator care are lock-uri interne.

    Args:
        sql_instance: SQL manager instance
        interval_seconds: Interval √Æntre rulƒÉri (default: 5 min)
    """
    aggregator = get_aggregator(sql_instance)

    logger.info(
        f"Starting periodic aggregation (interval={interval_seconds}s, "
        f"max_events={aggregator.config.max_events_per_batch})"
    )

    while True:
        try:
            result = aggregator.aggregate_all()

            if result["status"] == "success":
                logger.info(
                    f"‚úÖ Periodic aggregation: {result['events_processed']} events "
                    f"in {result['duration']:.2f}s"
                )
            elif result["status"] == "skipped":
                logger.debug(f"‚è≠Ô∏è  Periodic aggregation skipped: {result['reason']}")
            else:
                logger.error(f"‚ùå Periodic aggregation failed: {result.get('error')}")

        except Exception as e:
            logger.error(f"‚ùå Critical error in periodic aggregation: {e}", exc_info=True)

        time.sleep(interval_seconds)


# =============================================================================
# Testing & Debugging
# =============================================================================
if __name__ == "__main__":
    print("StatsAggregator Thread-Safe Version")
    print("=" * 70)
    print()
    print("‚úÖ Thread-safe with RLock")
    print("‚úÖ State persistence & recovery")
    print("‚úÖ Retry logic with exponential backoff")
    print("‚úÖ Metrics tracking")
    print("‚úÖ Configuration management")
    print()
    print("Ready for production use!")